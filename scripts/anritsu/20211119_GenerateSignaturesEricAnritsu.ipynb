{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M.0 Data initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import time\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://4b0dda8f9eed:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>discret_pierre_signature_generation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f178923acd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUMBER_OF_THREADS_TO_USE = \"*\"\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[' + NUMBER_OF_THREADS_TO_USE + ']') \\\n",
    "    .appName('discret_pierre_signature_generation') \\\n",
    "    .config('spark.driver.memory', '200g') \\\n",
    "    .config('spark.driver.maxResultSize', '15g') \\\n",
    "    .config('spark.rapids.sql.enabled','true') \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# those are the metadata used to sum the number of requests\n",
    "meta = ['LocationId', 'MinuteWithinWeek']\n",
    "\n",
    "# those are the metrics once we have gathered antennas and stuff\n",
    "#metrics = ['Voice','SMS_3G','PS','CS','Call','SMS_4G','Service_Req','HO']\n",
    "metrics = ['Call','SMS','Data','Mobility','Signalling','Emergency','Overload']\n",
    "\n",
    "SourceParquetFilesLoc = '/WORKSPACE/Pierre/exports/ANRITSU_dataset/Paris'\n",
    "MedianParquetFilesLoc = '/WORKSPACE/Pierre/exports/ANRITSU_meds/Paris'\n",
    "\n",
    "ParquetFilesSignaturesLoc = '/WORKSPACE/Pierre/exports/ANRITSU_sigs/Paris'\n",
    "ParquetFilesDistribsLoc = '/WORKSPACE/Pierre/exports/ANRITSU_distribs/Paris'\n",
    "\n",
    "ParquetFilesALRsLoc = '/WORKSPACE/Pierre/exports/ANRITSU_ALR/Paris'\n",
    "CsvFilesThresholdsLoc = '/WORKSPACE/Pierre/exports/ANRITSU_Paris_Thresholds.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/WORKSPACE/Pierre/exports/ANRITSU_sigs/Paris\n",
      "/WORKSPACE/Pierre/exports/ANRITSU_distribs/Paris\n",
      "/WORKSPACE/Pierre/exports/ANRITSU_ALR/Paris\n",
      "/WORKSPACE/Pierre/exports/ANRITSU_Paris_Thresholds.csv\n"
     ]
    }
   ],
   "source": [
    "print(ParquetFilesSignaturesLoc)\n",
    "print(ParquetFilesDistribsLoc)\n",
    "print(ParquetFilesALRsLoc)\n",
    "print(CsvFilesThresholdsLoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Load and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MinuteWithinWeek: integer (nullable = true)\n",
      " |-- Call: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- SMS: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- Data: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- Mobility: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- Signalling: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- Emergency: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- Overload: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- LocationId: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- time_local: timestamp (nullable = true)\n",
      " |-- Call: long (nullable = true)\n",
      " |-- SMS: long (nullable = true)\n",
      " |-- Data: long (nullable = true)\n",
      " |-- Mobility: long (nullable = true)\n",
      " |-- Signalling: long (nullable = true)\n",
      " |-- Emergency: long (nullable = true)\n",
      " |-- Overload: long (nullable = true)\n",
      " |-- MinuteWithinWeek: integer (nullable = true)\n",
      " |-- WeekOfYear: integer (nullable = true)\n",
      " |-- LocationId: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MediansDFSP = spark.read.parquet(MedianParquetFilesLoc)\n",
    "\n",
    "MediansDFSP.printSchema()\n",
    "\n",
    "OriginalDataDFSP = spark.read.parquet(SourceParquetFilesLoc)\n",
    "\n",
    "OriginalDataDFSP.printSchema()\n",
    "\n",
    "#Medians = MediansDFSP.toPandas()\n",
    "\n",
    "#MediansDFSP.show(10)\n",
    "\n",
    "#print(Medians.info(verbose=True))\n",
    "\n",
    "#lMinutes = []\n",
    "#for i in range(0,7*24*60):\n",
    "#    lMinutes.append([i])\n",
    "#dfMinutes = sc.parallelize(lMinutes).toDF([\"MinuteWithinWeek\"])\n",
    "#dfMinutes = dfMinutes.withColumn(\"MinuteWithinWeek\", dfMinutes.MinuteWithinWeek.cast('integer'))\n",
    "\n",
    "\n",
    "#MediansDF = pd.read_parquet(ParquetFilesLoc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 878 location groups\n",
      "All Weeks Ids:\n",
      "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "Train Weeks Ids:\n",
      "[9, 10, 11, 12, 13, 14, 15, 16, 22, 23, 24]\n"
     ]
    }
   ],
   "source": [
    "LocIdsList = sorted([x.LocationId for x in MediansDFSP.select('LocationId').distinct().collect()])\n",
    "\n",
    "print(\"found \" + str(len(LocIdsList)) + \" location groups\")\n",
    "\n",
    "#print(LocIdsList)\n",
    "\n",
    "AllWeeksIdsList = sorted([x.WeekOfYear for x in OriginalDataDFSP.select('WeekOfYear').distinct().collect()])\n",
    "\n",
    "print(\"All Weeks Ids:\")\n",
    "print(AllWeeksIdsList)\n",
    "\n",
    "TestWeeksIdsList = range(17,22)\n",
    "\n",
    "print(\"Test Weeks Ids:\")\n",
    "print(list(TestWeeksIdsList))\n",
    "\n",
    "TrainWeeksIdsList = list(set(AllWeeksIdsList) - set(TestWeeksIdsList))\n",
    "\n",
    "print(\"Train Weeks Ids:\")\n",
    "print(TrainWeeksIdsList)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Weeks Ids:\n",
      "[17, 18, 19, 20, 21]\n",
      "Train Weeks Ids:\n",
      "[9, 10, 11, 12, 13, 14, 15, 16, 22, 23, 24]\n"
     ]
    }
   ],
   "source": [
    "TestWeeksIdsList = range(17,22)\n",
    "\n",
    "print(\"Test Weeks Ids:\")\n",
    "print(list(TestWeeksIdsList))\n",
    "\n",
    "TrainWeeksIdsList = list(set(AllWeeksIdsList) - set(TestWeeksIdsList))\n",
    "\n",
    "print(\"Train Weeks Ids:\")\n",
    "print(TrainWeeksIdsList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# select only the medians\n",
    "for m in metrics:\n",
    "    MediansDFSP = MediansDFSP.withColumn(m, MediansDFSP[m].getItem(1))\n",
    "#MediansDFSP = MediansDFSP.select()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Signature extraction: 100%|██████████| 878/878 [02:00<00:00,  7.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from anr_discret import offlineMLbyPL\n",
    "\n",
    "filteredDFs = []\n",
    "\n",
    "\n",
    "#start_time = time.time()\n",
    "with tqdm(total=len(LocIdsList), desc='Signature extraction') as pbar:\n",
    "    # run 2 imbricated loops, this hasn't much consequence on the output thanks to the filter / partitioning thing \n",
    "    for LocId in LocIdsList:\n",
    "        #if LocId>10:\n",
    "        #    break\n",
    "        # some minutes may be missing. Since they are already sorted, we use MinuteWithinWeek as an index \n",
    "        # converting the filtered data to pandas dataframe seems to be actually quite fast ; using the index is straitforward too\n",
    "        df = MediansDFSP.filter(MediansDFSP.LocationId == LocId).toPandas().set_index('MinuteWithinWeek')\n",
    "\n",
    "        # fill the missing minutes ; don't forget to set the proper value to columns weeksgroup and locationid\n",
    "        df = df.reindex(range(0,24*7*60), fill_value=0).assign(LocationId=LocId)\n",
    "\n",
    "        # now the DataFrame is ready to compute the butterworth filter\n",
    "        filt = offlineMLbyPL.signature_filtered(df, cutoff=8, metrics=metrics).reset_index()\n",
    "        # reset index because we want the \"minutewithinweek\" column back as a standard column\n",
    "\n",
    "        # use the append function not to slow down computation because of the concatenation thing\n",
    "        filteredDFs.append(filt)\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "            \n",
    "# just concat everything at the same time\n",
    "FilteredSignatureDF = pd.concat(filteredDFs)\n",
    "\n",
    "# save as parquet because it's much more efficient - keep the same partition structure although it's not really good\n",
    "# don't forget to remove the \"index thing\" (although we know that the actual index is ['WeeksGroup','LocationId', 'MinuteWithinWeek'])\n",
    "#FilteredSignatureDF.to_parquet(path=ParquetFilesSignaturesLoc, partition_cols=['WeeksGroup','LocationId'], index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "FilteredSignatureDF.to_parquet(path=ParquetFilesSignaturesLoc, partition_cols=['LocationId'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Signature extraction: 100%|██████████| 878/878 [34:41<00:00,  2.37s/it]\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "SigmaSep = 2.32          # how do we select the set of absolute error values to \n",
    "\n",
    "\n",
    "def fit_distribution_eric_local(df:pd.DataFrame, metrics:list) -> dict:\n",
    "    \"\"\"Fit a Gamma distribution to the given columns the input dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.core.frame.DataFrame\n",
    "        The input dataframe (usually containing AE values)\n",
    "    metrics: list\n",
    "        The list of column names to fit the Gamma distribution\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    distrib: dict\n",
    "        The dictionary containing, for each column name, the Gamma parameters\n",
    "    \"\"\"\n",
    "\n",
    "    distrib = {}\n",
    "\n",
    "    #compter nombre de cas où on est à np.inf pour vérifier que ça n'arrive pas\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    \n",
    "    for m in metrics:\n",
    "        # pour éviter sort : hypothèse distribution gaussienne + estimer à partir de mean et std\n",
    "        # alpha = 2.32 std pour exclure 99% des donnée\n",
    "        # alpha = 1.5 std pour exclure 95% données\n",
    "        x_mean = df[m].mean()\n",
    "        x_std = df[m].std()\n",
    "        SeparationThresh = x_mean + SigmaSep * x_std\n",
    "        x = df[m].loc[df[m] > SeparationThresh]\n",
    "        \n",
    "        # tout écart inférieur à seuil aura comme probabilité 1-proba\n",
    "        proba = len(x.index) / len(df.index)\n",
    "        \n",
    "        # donc on stocke seuil et on compare dans la phase de détection à seuil\n",
    "        # si value < seuil, on retourne 1-proba\n",
    "        # sinon, on regarde la sf de la loi et on multiplie par proba\n",
    "        # regarder la manière dont cette loi est respectée\n",
    "        firsttuple = (SeparationThresh, proba, len(x.index))\n",
    "\n",
    "        if len(x.index)>=3:\n",
    "            # regarder options pour supprimer outliers dans la fonction gamma fit\n",
    "            fittuple = stats.gamma.fit(x)\n",
    "            # before the code used to translate this tuple used to be the following:\n",
    "\n",
    "            distrib[m] = firsttuple + (fittuple[0], fittuple[-2], fittuple[-1])\n",
    "        else:\n",
    "            distrib[m] = firsttuple + (np.nan, np.nan, np.nan)\n",
    "    return distrib\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "totalALR = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MinALSThreshold = 1 / (60*24*365.25*10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_alr_eric_local(df:pd.DataFrame, distrib:dict, metrics:list) -> pd.DataFrame:\n",
    "    \"\"\"Computes the Anomaly Likelihood Rate (ALR) over the input dataframe.\n",
    "    The ALR corresponds to the sum of the logs of the p-value for each service data.\n",
    "    The p-value is obtained for each service data by fitting a Gamma distribution over the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.core.frame.DataFrame\n",
    "        The input dataframe (usually containing AE values)\n",
    "    distrib: dict\n",
    "        The error distribution parameters for all services\n",
    "    metrics: list\n",
    "        The list of column names corresponding to the service data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df: pandas.core.frame.DataFrame\n",
    "        The same dataframe with the additional ALR column\n",
    "    \"\"\"\n",
    "\n",
    "    res = pd.DataFrame().reindex_like(df)\n",
    "    als = pd.DataFrame(index = res.index, columns = metrics)\n",
    "    \n",
    "    CopyCols = list(set(df.columns) - set(metrics))\n",
    "    res[CopyCols] = df[CopyCols].copy(deep=True)\n",
    "\n",
    "    for m in metrics:\n",
    "        mLoc = df.columns.get_loc(m)\n",
    "            \n",
    "        SeparationThresh = distrib.loc['thresh',m]\n",
    "        proba = distrib.loc['proba',m]\n",
    "        nvalues = distrib.loc['nvalues',m]\n",
    "        arg = distrib.loc['k',m]\n",
    "        loc = distrib.loc['loc',m]\n",
    "        scale = distrib.loc['theta',m]\n",
    "\n",
    "        if nvalues<3:\n",
    "            #print(\"case where the gamma law was not fitted on metric \" + m)\n",
    "            res.loc[:,m] = pd.Series(np.nan, index=df.index)\n",
    "        else:\n",
    "            # default value is 1-proba\n",
    "            res.loc[:,m] = pd.Series((1. - proba), index=df.index)\n",
    "            # the gamma law was fitted on this metric\n",
    "            indexThresh = df.index[df[m]>SeparationThresh]\n",
    "\n",
    "            res.loc[indexThresh,m] = (1. - proba) * stats.gamma.sf(df.loc[indexThresh, m], arg, loc=loc, scale=scale)\n",
    "            res.loc[indexThresh,m].clip(lower=MinALSThreshold, inplace=True)\n",
    "\n",
    "        als[m] = pd.Series(np.log(res[m]), index=res.index)\n",
    "\n",
    "    res['ALR'] = als.sum(axis=1)\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "signatureDFs = []\n",
    "\n",
    "ColumnsDropList = []\n",
    "for m in metrics:\n",
    "    ColumnsDropList.append(m+'_ref')\n",
    "\n",
    "count = 0\n",
    "\n",
    "distributionsDFList = []\n",
    "\n",
    "\n",
    "\n",
    "#SmallerLocIdsList = LocIdsList[0:50]\n",
    "\n",
    "    \n",
    "with tqdm(total=len(LocIdsList), desc='Signature extraction') as pbar:\n",
    "    # run 2 imbricated loops, this hasn't much consequence on the output thanks to the filter / partitioning thing \n",
    "    for LocId in LocIdsList:\n",
    "        #if LocId>10:\n",
    "        #    break\n",
    "        # gather all the data corresponding to the training group and the corresponding location\n",
    "        # start with the location\n",
    "        #OriginalDataLocSP = OriginalDataDFSP.filter(OriginalDataDFSP.LocationId == LocId)\n",
    "\n",
    "\n",
    "        #start_time = time.time()\n",
    "        referenceDF = FilteredSignatureDF.loc[(FilteredSignatureDF['LocationId']==LocId)].drop(columns=['LocationId'])\n",
    "        #print(\"grabbing the ref time performed in \" + str((time.time() - start_time)) + \" seconds\")\n",
    "\n",
    "        # then the corresponding weeks. I believe that it's more convenient by unioning stuff\n",
    "        #start_time = time.time()\n",
    "\n",
    "        #OriginalDataDFList = []\n",
    "        ErrorsDFsList = []\n",
    "        for wk in TrainWeeksIdsList:\n",
    "            wkDF = OriginalDataDFSP.drop('time_local').filter((OriginalDataDFSP.WeekOfYear==wk) & (OriginalDataDFSP.LocationId == LocId)).toPandas()\n",
    "            wkDF = wkDF.set_index('MinuteWithinWeek')\n",
    "            wkDF = wkDF.reindex(range(0,24*7*60), fill_value=0).assign(WeekOfYear=wk, LocationId=LocId).fillna(0)\n",
    "\n",
    "            for m in metrics:\n",
    "                # \n",
    "                wkDF[m] = wkDF[m] - referenceDF[m]\n",
    "                #wkDF[m] = abs(wkDF[m] - referenceDF[m])\n",
    "\n",
    "            wkDF.reset_index(inplace=True)\n",
    "\n",
    "            #print(\"length wkDF :\" + str(len(wkDF.index)))\n",
    "\n",
    "            ErrorsDFsList.append( wkDF )\n",
    "\n",
    "        errors = pd.concat(ErrorsDFsList)\n",
    "\n",
    "\n",
    "        #distributions = offlineMLbyPL.get_distrib_params(errors, meta=meta, metrics=metrics)\n",
    "        distribution = pd.DataFrame(np.nan, index=['thresh', 'proba', 'nvalues', 'k','loc','theta'], columns=['LocationId', *metrics])\n",
    "        distribution = distribution.assign(LocationId=LocId)\n",
    "        distrib = fit_distribution_eric_local(errors, metrics)\n",
    "        for m in metrics:\n",
    "            for k in range(0,6):\n",
    "                mLoc = distribution.columns.get_loc(m)\n",
    "                distribution.iloc[k,mLoc] = distrib[m][k]\n",
    "\n",
    "\n",
    "        distribution.index.names = ['GammaParam']\n",
    "\n",
    "        #alrDF = compute_alr_eric_local(AbsErrors, GammaLawDF, metrics)\n",
    "        alrDF = compute_alr_eric_local(errors, distribution, metrics)\n",
    "\n",
    "        distribution.reset_index(inplace=True)\n",
    "        distributionsDFList.append(distribution)\n",
    "\n",
    "\n",
    "        totalALR.append(alrDF)\n",
    "\n",
    "        #break\n",
    "\n",
    "\n",
    "        pbar.update(1)\n",
    "        \n",
    "            \n",
    "distributionsDF = pd.concat(distributionsDFList)\n",
    "totalALRDF = pd.concat(totalALR)\n",
    "\n",
    "\n",
    "\n",
    "#print(distributionsDF.info())\n",
    "#print(distributionsDF.describe())\n",
    "#print(distributionsDF)\n",
    "\n",
    "\n",
    "#distributionsDF.to_parquet(path=ParquetFilesDistribsLoc, partition_cols=['WeeksGroup','LocationId'], index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ParquetFilesALRsLoc = '/WORKSPACE/Pierre/tests/' + DatasetPrefix + '_' + LocationPrefix + '_ALR0fill300LocIdsGroup012/'\n",
    "distributionsDF.to_parquet(path=ParquetFilesDistribsLoc, partition_cols=['LocationId'], index=False)\n",
    "\n",
    "#ParquetFilesDistribsLoc = '/WORKSPACE/Pierre/tests/' + DatasetPrefix + '_' + LocationPrefix + '_distribs300LocIdsGroup012/'\n",
    "totalALRDF.to_parquet(path=ParquetFilesALRsLoc, partition_cols=['LocationId'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def level1byPL(alr:pd.Series) -> float:\n",
    "    \"\"\"Sets a level 1 ALR threshold (i.e. pre-alert threshold) for the input data series.\n",
    "    The threshold corresponds to the 2-sigma quantile of the ALR distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alr: pandas.core.frame.Series\n",
    "        The series of ALR values\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    thresh: float\n",
    "        The ALR threshold of the series\n",
    "    \"\"\"\n",
    "\n",
    "    #res = alr.replace([np.inf, -np.inf], np.nan).dropna(how=\"all\")\n",
    "    # 1 alert every 22 minutes was the parameter used by \n",
    "    # once per 4 hours -> 1 per 4*60 minutes\n",
    "    #return res.quantile(1/240)\n",
    "    return alr.quantile(0.00416667)\n",
    "\n",
    "def level2byPL(alr:pd.Series) -> float:\n",
    "    # 1 alert every 370 minutes\n",
    "    \"\"\"Sets a level 2 ALR threshold (i.e. alert threshold) for the input data series.\n",
    "    The threshold corresponds to the 3-sigma quantile of the ALR distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alr: pandas.core.frame.Series\n",
    "        The series of ALR values\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    thresh: float\n",
    "        The ALR threshold of the series\n",
    "    \"\"\"\n",
    "\n",
    "    #res = alr.replace([np.inf, -np.inf], np.nan).dropna(how=\"all\")\n",
    "    #return res.quantile(1-0.9973)\n",
    "    # once per day -> 1 per 24*60 minutes\n",
    "    #return res.quantile(1/1440)\n",
    "    return alr.quantile(0.0006944)\n",
    "\n",
    "def level3byPL(alr:pd.Series) -> float:\n",
    "    \"\"\"Sets a level 3 ALR threshold (i.e. maximal alert threshold) for the input data series.\n",
    "    The threshold corresponds to the 4-sigma quantile of the ALR distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alr: pandas.core.frame.Series\n",
    "        The series of ALR values\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    thresh: float\n",
    "        The ALR threshold of the series\n",
    "    \"\"\"\n",
    "    #1 alert every 15873 minute\n",
    "    #res = alr.replace([np.inf, -np.inf], np.nan).dropna(how=\"all\")\n",
    "    #return res.quantile(1-0.999937)\n",
    "    return alr.quantile(0.0000992)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#CsvFilesThresholdsLoc = 'exports/' + DatasetPrefix + '_thresholds_0fill2groups_' + LocationPrefix + '.csv'\n",
    "\n",
    "\n",
    "# apply the offlineML functions to aggregate thresholds for each antenna\n",
    "thresholds = totalALRDF.groupby(['LocationId']).ALR.agg([level1byPL, level2byPL, level3byPL])\n",
    "thresholds.reset_index(inplace=True)\n",
    "\n",
    "# don't forget to store the index or the following will be annoying\n",
    "#thresholds.reset_index(inplace=True)\n",
    "#thresholds.to_csv(CsvFilesThresholdsLoc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CsvFilesThresholdsLoc = '/WORKSPACE/Pierre/tests/' + DatasetPrefix + '_' + LocationPrefix + '_thresholds_300LocIdsGroup012.csv'\n",
    "thresholds.to_csv(CsvFilesThresholdsLoc, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 878 entries, 0 to 877\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   LocationId  878 non-null    int64  \n",
      " 1   level1byPL  878 non-null    float64\n",
      " 2   level2byPL  878 non-null    float64\n",
      " 3   level3byPL  878 non-null    float64\n",
      "dtypes: float64(3), int64(1)\n",
      "memory usage: 27.6 KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationId</th>\n",
       "      <th>level1byPL</th>\n",
       "      <th>level2byPL</th>\n",
       "      <th>level3byPL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>878.000000</td>\n",
       "      <td>878.000000</td>\n",
       "      <td>878.000000</td>\n",
       "      <td>878.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14191.920273</td>\n",
       "      <td>-11.290576</td>\n",
       "      <td>-22.958969</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8216.132209</td>\n",
       "      <td>14.546115</td>\n",
       "      <td>34.008392</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>99.000000</td>\n",
       "      <td>-195.910663</td>\n",
       "      <td>-611.452600</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6773.250000</td>\n",
       "      <td>-11.136866</td>\n",
       "      <td>-22.593435</td>\n",
       "      <td>-37.063879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>14359.000000</td>\n",
       "      <td>-7.439725</td>\n",
       "      <td>-14.464153</td>\n",
       "      <td>-24.246616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>21358.500000</td>\n",
       "      <td>-5.727931</td>\n",
       "      <td>-10.979496</td>\n",
       "      <td>-17.432521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>27972.000000</td>\n",
       "      <td>-0.002707</td>\n",
       "      <td>-1.196029</td>\n",
       "      <td>-3.059375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         LocationId  level1byPL  level2byPL  level3byPL\n",
       "count    878.000000  878.000000  878.000000  878.000000\n",
       "mean   14191.920273  -11.290576  -22.958969        -inf\n",
       "std     8216.132209   14.546115   34.008392         NaN\n",
       "min       99.000000 -195.910663 -611.452600        -inf\n",
       "25%     6773.250000  -11.136866  -22.593435  -37.063879\n",
       "50%    14359.000000   -7.439725  -14.464153  -24.246616\n",
       "75%    21358.500000   -5.727931  -10.979496  -17.432521\n",
       "max    27972.000000   -0.002707   -1.196029   -3.059375"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(thresholds.info(verbose=True))\n",
    "thresholds.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 50 entries, 5 to 5\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   GammaParam   50 non-null     object \n",
      " 1   LocationId   50 non-null     int64  \n",
      " 2   WeeksGroup   50 non-null     int64  \n",
      " 3   Voice        50 non-null     float64\n",
      " 4   SMS_3G       50 non-null     float64\n",
      " 5   PS           50 non-null     float64\n",
      " 6   CS           50 non-null     float64\n",
      " 7   Call         25 non-null     float64\n",
      " 8   SMS_4G       25 non-null     float64\n",
      " 9   Service_Req  25 non-null     float64\n",
      " 10  HO           25 non-null     float64\n",
      "dtypes: float64(8), int64(2), object(1)\n",
      "memory usage: 4.7+ KB\n",
      "\n",
      "\n",
      "proba statistics\n",
      "           Voice     SMS_3G         PS         CS       Call     SMS_4G  Service_Req         HO\n",
      "count  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000    50.000000  50.000000\n",
      "mean    0.037998   0.031231   0.025220   0.023378   0.018931   0.016615     0.017226   0.017284\n",
      "std     0.011358   0.008655   0.011064   0.011580   0.020244   0.017558     0.019831   0.017966\n",
      "min     0.008929   0.001797   0.003395   0.001775   0.000000   0.000000     0.000000   0.000000\n",
      "25%     0.031313   0.026505   0.017267   0.014658   0.000000   0.000000     0.000000   0.000000\n",
      "50%     0.038040   0.032865   0.025028   0.023760   0.006785   0.006707     0.001306   0.010863\n",
      "75%     0.044431   0.037282   0.033292   0.032529   0.035670   0.032115     0.036671   0.036654\n",
      "max     0.068397   0.044676   0.045888   0.044874   0.064253   0.047178     0.054773   0.045811\n",
      "\n",
      "\n",
      "possible non-zero values depending on the technology\n",
      "TECHNO\n",
      "3G    50\n",
      "4G    25\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#print(LocIdsList)\n",
    "# some verifications about what we have in the \n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "distributionsDF[distributionsDF['GammaParam']=='theta'].info()\n",
    "\n",
    "#print(\"threshold statistics\")\n",
    "#distributionsDF[distributionsDF['GammaParam']=='thresh'].drop(columns=['LocationId', 'WeeksGroup']).describe()\n",
    "\n",
    "print(\"\\n\\nproba statistics\")\n",
    "print(distributionsDF[distributionsDF['GammaParam']=='proba'].drop(columns=['LocationId', 'WeeksGroup']).describe())\n",
    "\n",
    "LocIdsInfosDF = pd.read_csv('exports/Cancan_Paris_LocInfos_AllData.csv')\n",
    "#print(LocIdsInfosDF.info(verbose=True))\n",
    "\n",
    "SelectedLocIdsInfos = LocIdsInfosDF[LocIdsInfosDF['LocationId'].isin(SmallerLocIdsList)]\n",
    "#print(SelectedLocIdsInfos.info(verbose=True))\n",
    "\n",
    "print(\"\\n\\npossible non-zero values depending on the technology\")\n",
    "CountTechno = SelectedLocIdsInfos.groupby(['LocationId', 'TECHNO']).size().groupby('TECHNO').count()\n",
    "print(CountTechno)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationId</th>\n",
       "      <th>WeeksGroup</th>\n",
       "      <th>level1byPL</th>\n",
       "      <th>level2byPL</th>\n",
       "      <th>level3byPL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.0</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1061.803333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.195179</td>\n",
       "      <td>-11.174876</td>\n",
       "      <td>-18.086576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>110.215135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.778842</td>\n",
       "      <td>6.997304</td>\n",
       "      <td>9.384837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>875.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-26.323358</td>\n",
       "      <td>-56.610089</td>\n",
       "      <td>-65.210885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>961.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.574128</td>\n",
       "      <td>-14.145312</td>\n",
       "      <td>-22.871932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1064.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.398969</td>\n",
       "      <td>-9.676692</td>\n",
       "      <td>-16.536602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1156.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.668047</td>\n",
       "      <td>-6.774801</td>\n",
       "      <td>-11.970358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1251.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000121</td>\n",
       "      <td>-0.000121</td>\n",
       "      <td>-0.000123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        LocationId  WeeksGroup  level1byPL  level2byPL  level3byPL\n",
       "count   300.000000       300.0  300.000000  300.000000  300.000000\n",
       "mean   1061.803333         0.0   -5.195179  -11.174876  -18.086576\n",
       "std     110.215135         0.0    3.778842    6.997304    9.384837\n",
       "min     875.000000         0.0  -26.323358  -56.610089  -65.210885\n",
       "25%     961.750000         0.0   -6.574128  -14.145312  -22.871932\n",
       "50%    1064.000000         0.0   -4.398969   -9.676692  -16.536602\n",
       "75%    1156.500000         0.0   -2.668047   -6.774801  -11.970358\n",
       "max    1251.000000         0.0   -0.000121   -0.000121   -0.000123"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
