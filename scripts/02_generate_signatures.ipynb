{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M.0 Data initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import time\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://4b0dda8f9eed:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>discret_pierre_signature_generation</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6d852dc050>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUMBER_OF_THREADS_TO_USE = \"*\"\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[' + NUMBER_OF_THREADS_TO_USE + ']') \\\n",
    "    .appName('discret_pierre_signature_generation') \\\n",
    "    .config('spark.driver.memory', '200g') \\\n",
    "    .config('spark.driver.maxResultSize', '15g') \\\n",
    "    .config('spark.rapids.sql.enabled','true') \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# those are the metadata used to sum the number of requests\n",
    "meta = ['WeeksGroup', 'LocationId', 'MinuteWithinWeek']\n",
    "\n",
    "# those are the metrics once we have gathered antennas and stuff\n",
    "metrics = ['Voice','SMS_3G','PS','CS','Call','SMS_4G','Service_Req','HO']\n",
    "#metrics = ['Call','SMS','Data','Mobility','Signalling','Emergency','Overload']\n",
    "\n",
    "SourceParquetFilesLoc = '/WORKSPACE/Pierre/Cancan2022/Cancan2022_Paris/'\n",
    "MedianParquetFilesLoc = '/WORKSPACE/Pierre/Cancan2022/Cancan2022_Paris_meds/'\n",
    "\n",
    "ParquetFilesSignaturesLoc = '/WORKSPACE/Pierre/Cancan2022/Cancan2022_Paris_sigs/'\n",
    "ParquetFilesDistribsLoc = '/WORKSPACE/Pierre/Cancan2022/Cancan2022_Paris_distribs/'\n",
    "\n",
    "ParquetFilesALRsLoc = '/WORKSPACE/Pierre/Cancan2022/Cancan2022_Paris_ALR/'\n",
    "CsvFilesThresholdsLoc = '/WORKSPACE/Pierre/Cancan2022/Cancan2022_Paris_Thresholds.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/WORKSPACE/Pierre/Cancan2022/Cancan2022_Paris_sigs/\n",
      "/WORKSPACE/Pierre/Cancan2022/Cancan2022_Paris_distribs/\n",
      "/WORKSPACE/Pierre/Cancan2022/Cancan2022_Paris_ALR/\n",
      "/WORKSPACE/Pierre/Cancan2022/Cancan2022_Paris_Thresholds.csv\n"
     ]
    }
   ],
   "source": [
    "print(ParquetFilesSignaturesLoc)\n",
    "print(ParquetFilesDistribsLoc)\n",
    "print(ParquetFilesALRsLoc)\n",
    "print(CsvFilesThresholdsLoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Load and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MinuteWithinWeek: integer (nullable = true)\n",
      " |-- Voice: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- SMS_3G: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- PS: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- CS: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- Call: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- SMS_4G: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- Service_Req: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- HO: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- WeeksGroup: integer (nullable = true)\n",
      " |-- LocationId: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- time_utc: string (nullable = true)\n",
      " |-- time_local: timestamp (nullable = true)\n",
      " |-- Voice: long (nullable = true)\n",
      " |-- PS: long (nullable = true)\n",
      " |-- SMS_3G: long (nullable = true)\n",
      " |-- CS: long (nullable = true)\n",
      " |-- Service_Req: long (nullable = true)\n",
      " |-- Call: long (nullable = true)\n",
      " |-- SMS_4G: long (nullable = true)\n",
      " |-- HO: long (nullable = true)\n",
      " |-- MinuteWithinWeek: integer (nullable = true)\n",
      " |-- WeeksGroup: integer (nullable = true)\n",
      " |-- WeekOfYear: integer (nullable = true)\n",
      " |-- LocationId: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MediansDFSP = spark.read.parquet(MedianParquetFilesLoc)\n",
    "\n",
    "MediansDFSP.printSchema()\n",
    "\n",
    "OriginalDataDFSP = spark.read.parquet(SourceParquetFilesLoc)\n",
    "\n",
    "OriginalDataDFSP.printSchema()\n",
    "\n",
    "#Medians = MediansDFSP.toPandas()\n",
    "\n",
    "#MediansDFSP.show(10)\n",
    "\n",
    "#print(Medians.info(verbose=True))\n",
    "\n",
    "#lMinutes = []\n",
    "#for i in range(0,7*24*60):\n",
    "#    lMinutes.append([i])\n",
    "#dfMinutes = sc.parallelize(lMinutes).toDF([\"MinuteWithinWeek\"])\n",
    "#dfMinutes = dfMinutes.withColumn(\"MinuteWithinWeek\", dfMinutes.MinuteWithinWeek.cast('integer'))\n",
    "\n",
    "\n",
    "#MediansDF = pd.read_parquet(ParquetFilesLoc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1026 location groups\n",
      "All Weeks Ids:\n",
      "[11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "All Weeks Ids:\n",
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "LocIdsList = sorted([x.LocationId for x in MediansDFSP.select('LocationId').distinct().collect()])\n",
    "\n",
    "print(\"found \" + str(len(LocIdsList)) + \" location groups\")\n",
    "\n",
    "#print(LocIdsList)\n",
    "\n",
    "AllWeeksIdsList = sorted([x.WeekOfYear for x in OriginalDataDFSP.select('WeekOfYear').distinct().collect()])\n",
    "\n",
    "print(\"All Weeks Ids:\")\n",
    "print(AllWeeksIdsList)\n",
    "\n",
    "\n",
    "AllWeeksGroupsList = sorted([x.WeeksGroup for x in OriginalDataDFSP.select('WeeksGroup').distinct().collect()])\n",
    "\n",
    "print(\"All Weeks Ids:\")\n",
    "print(AllWeeksGroupsList)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# select only the medians\n",
    "for m in metrics:\n",
    "    MediansDFSP = MediansDFSP.withColumn(m, MediansDFSP[m].getItem(1))\n",
    "#MediansDFSP = MediansDFSP.select()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Signature extraction: 100%|██████████| 3078/3078 [07:09<00:00,  7.16it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from anr_discret import offlineMLbyPL\n",
    "\n",
    "filteredDFs = []\n",
    "\n",
    "\n",
    "#start_time = time.time()\n",
    "with tqdm(total=len(LocIdsList)*len(AllWeeksGroupsList), desc='Signature extraction') as pbar:\n",
    "    for WGrp in AllWeeksGroupsList:\n",
    "    \n",
    "        # run 2 imbricated loops, this hasn't much consequence on the output thanks to the filter / partitioning thing \n",
    "        for LocId in LocIdsList:\n",
    "            #if LocId>10:\n",
    "            #    break\n",
    "            # some minutes may be missing. Since they are already sorted, we use MinuteWithinWeek as an index \n",
    "            # converting the filtered data to pandas dataframe seems to be actually quite fast ; using the index is straitforward too\n",
    "            df = MediansDFSP.filter((MediansDFSP.LocationId == LocId) & (MediansDFSP.WeeksGroup == WGrp)).toPandas().set_index('MinuteWithinWeek')\n",
    "\n",
    "            # fill the missing minutes ; don't forget to set the proper value to columns weeksgroup and locationid\n",
    "            df = df.reindex(range(0,24*7*60), fill_value=0).assign(LocationId=LocId, WeeksGroup=WGrp)\n",
    "\n",
    "            # now the DataFrame is ready to compute the butterworth filter\n",
    "            filt = offlineMLbyPL.signature_filtered(df, cutoff=8, metrics=metrics).reset_index()\n",
    "            # reset index because we want the \"minutewithinweek\" column back as a standard column\n",
    "\n",
    "            # use the append function not to slow down computation because of the concatenation thing\n",
    "            filteredDFs.append(filt)\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "            \n",
    "# just concat everything at the same time\n",
    "FilteredSignatureDF = pd.concat(filteredDFs)\n",
    "\n",
    "# save as parquet because it's much more efficient - keep the same partition structure although it's not really good\n",
    "# don't forget to remove the \"index thing\" (although we know that the actual index is ['WeeksGroup','LocationId', 'MinuteWithinWeek'])\n",
    "#FilteredSignatureDF.to_parquet(path=ParquetFilesSignaturesLoc, partition_cols=['WeeksGroup','LocationId'], index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FilteredSignatureDF.to_parquet(path=ParquetFilesSignaturesLoc, partition_cols=['WeeksGroup','LocationId'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Signature extraction: 100%|██████████| 1026/1026 [27:59<00:00,  1.64s/it]\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "SigmaSep = 2.32          # how do we select the set of absolute error values to \n",
    "\n",
    "\n",
    "def fit_distribution_eric_local(df:pd.DataFrame, metrics:list) -> dict:\n",
    "    \"\"\"Fit a Gamma distribution to the given columns the input dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.core.frame.DataFrame\n",
    "        The input dataframe (usually containing AE values)\n",
    "    metrics: list\n",
    "        The list of column names to fit the Gamma distribution\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    distrib: dict\n",
    "        The dictionary containing, for each column name, the Gamma parameters\n",
    "    \"\"\"\n",
    "\n",
    "    distrib = {}\n",
    "\n",
    "    #compter nombre de cas où on est à np.inf pour vérifier que ça n'arrive pas\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    \n",
    "    for m in metrics:\n",
    "        # pour éviter sort : hypothèse distribution gaussienne + estimer à partir de mean et std\n",
    "        # alpha = 2.32 std pour exclure 99% des donnée\n",
    "        # alpha = 1.5 std pour exclure 95% données\n",
    "        x_mean = df[m].mean()\n",
    "        x_std = df[m].std()\n",
    "        SeparationThresh = x_mean + SigmaSep * x_std\n",
    "        x = df[m].loc[df[m] > SeparationThresh]\n",
    "        \n",
    "        # tout écart inférieur à seuil aura comme probabilité 1-proba\n",
    "        proba = len(x.index) / len(df.index)\n",
    "        \n",
    "        # donc on stocke seuil et on compare dans la phase de détection à seuil\n",
    "        # si value < seuil, on retourne 1-proba\n",
    "        # sinon, on regarde la sf de la loi et on multiplie par proba\n",
    "        # regarder la manière dont cette loi est respectée\n",
    "        firsttuple = (SeparationThresh, proba, len(x.index))\n",
    "\n",
    "        if len(x.index)>=3:\n",
    "            # regarder options pour supprimer outliers dans la fonction gamma fit\n",
    "            fittuple = stats.gamma.fit(x)\n",
    "            # before the code used to translate this tuple used to be the following:\n",
    "\n",
    "            distrib[m] = firsttuple + (fittuple[0], fittuple[-2], fittuple[-1])\n",
    "        else:\n",
    "            distrib[m] = firsttuple + (np.nan, np.nan, np.nan)\n",
    "    return distrib\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "totalALR = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MinALSThreshold = 1 / (60*24*365.25*10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_alr_eric_local(df:pd.DataFrame, distrib:dict, metrics:list) -> pd.DataFrame:\n",
    "    \"\"\"Computes the Anomaly Likelihood Rate (ALR) over the input dataframe.\n",
    "    The ALR corresponds to the sum of the logs of the p-value for each service data.\n",
    "    The p-value is obtained for each service data by fitting a Gamma distribution over the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.core.frame.DataFrame\n",
    "        The input dataframe (usually containing AE values)\n",
    "    distrib: dict\n",
    "        The error distribution parameters for all services\n",
    "    metrics: list\n",
    "        The list of column names corresponding to the service data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df: pandas.core.frame.DataFrame\n",
    "        The same dataframe with the additional ALR column\n",
    "    \"\"\"\n",
    "\n",
    "    res = pd.DataFrame().reindex_like(df)\n",
    "    als = pd.DataFrame(index = res.index, columns = metrics)\n",
    "    \n",
    "    CopyCols = list(set(df.columns) - set(metrics))\n",
    "    res[CopyCols] = df[CopyCols].copy(deep=True)\n",
    "\n",
    "    for m in metrics:\n",
    "        mLoc = df.columns.get_loc(m)\n",
    "            \n",
    "        SeparationThresh = distrib.loc['thresh',m]\n",
    "        proba = distrib.loc['proba',m]\n",
    "        nvalues = distrib.loc['nvalues',m]\n",
    "        arg = distrib.loc['k',m]\n",
    "        loc = distrib.loc['loc',m]\n",
    "        scale = distrib.loc['theta',m]\n",
    "\n",
    "        if nvalues<3:\n",
    "            #print(\"case where the gamma law was not fitted on metric \" + m)\n",
    "            res.loc[:,m] = pd.Series(np.nan, index=df.index)\n",
    "        else:\n",
    "            # default value is 1-proba\n",
    "            res.loc[:,m] = pd.Series((1. - proba), index=df.index)\n",
    "            # the gamma law was fitted on this metric\n",
    "            indexThresh = df.index[df[m]>SeparationThresh]\n",
    "\n",
    "            res.loc[indexThresh,m] = (1. - proba) * stats.gamma.sf(df.loc[indexThresh, m], arg, loc=loc, scale=scale)\n",
    "            res.loc[indexThresh,m].clip(lower=MinALSThreshold, inplace=True)\n",
    "\n",
    "        als[m] = pd.Series(np.log(res[m]), index=res.index)\n",
    "\n",
    "    res['ALR'] = als.sum(axis=1)\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "signatureDFs = []\n",
    "\n",
    "ColumnsDropList = []\n",
    "for m in metrics:\n",
    "    ColumnsDropList.append(m+'_ref')\n",
    "\n",
    "count = 0\n",
    "\n",
    "distributionsDFList = []\n",
    "\n",
    "\n",
    "\n",
    "#SmallerLocIdsList = LocIdsList[0:50]\n",
    "\n",
    "    \n",
    "#with tqdm(total=len(LocIdsList)*len(AllWeeksGroupsList), desc='Signature extraction') as pbar:\n",
    "with tqdm(total=len(LocIdsList)*1, desc='Signature extraction') as pbar:\n",
    "        \n",
    "    # run 2 imbricated loops, this hasn't much consequence on the output thanks to the filter / partitioning thing \n",
    "    #for WGrp in AllWeeksGroupsList:\n",
    "    for WGrp in range(2,3):\n",
    "        \n",
    "        GroupDFSP = OriginalDataDFSP.drop('time_local').filter(OriginalDataDFSP.WeeksGroup==WGrp)\n",
    "        TestWeeksIdsList = sorted([x.WeekOfYear for x in GroupDFSP.select('WeekOfYear').distinct().collect()])\n",
    "        TrainWeeksIdsList = list(set(AllWeeksIdsList) - set(TestWeeksIdsList))\n",
    "                                                               \n",
    "        \n",
    "        for LocId in LocIdsList:\n",
    "            #if LocId>20:\n",
    "            #    break\n",
    "            # gather all the data corresponding to the training group and the corresponding location\n",
    "            # start with the location\n",
    "            OriginalDataLocSP = OriginalDataDFSP.filter(OriginalDataDFSP.LocationId == LocId)\n",
    "\n",
    "\n",
    "            #start_time = time.time()\n",
    "            referenceDF = FilteredSignatureDF.loc[(FilteredSignatureDF['LocationId']==LocId) & (FilteredSignatureDF['WeeksGroup']==WGrp)].drop(columns=['LocationId','WeeksGroup'])\n",
    "            #print(\"grabbing the ref time performed in \" + str((time.time() - start_time)) + \" seconds\")\n",
    "\n",
    "            # then the corresponding weeks. I believe that it's more convenient by unioning stuff\n",
    "            #start_time = time.time()\n",
    "\n",
    "            #OriginalDataDFList = []\n",
    "            ErrorsDFsList = []\n",
    "            for wk in TrainWeeksIdsList:\n",
    "                wkDF = OriginalDataLocSP.filter(OriginalDataLocSP.WeekOfYear==wk).drop('time_utc', 'time_local').toPandas()\n",
    "                wkDF = wkDF.set_index('MinuteWithinWeek')\n",
    "                wkDF = wkDF.reindex(range(0,24*7*60), fill_value=0).assign(WeekOfYear=wk, LocationId=LocId, WeeksGroup=WGrp).fillna(0)\n",
    "\n",
    "                for m in metrics:\n",
    "                    wkDF[m] = wkDF[m] - referenceDF[m]\n",
    "                    #wkDF[m] = abs(wkDF[m] - referenceDF[m])\n",
    "\n",
    "                wkDF.reset_index(inplace=True)\n",
    "\n",
    "                #print(\"length wkDF :\" + str(len(wkDF.index)))\n",
    "\n",
    "                ErrorsDFsList.append( wkDF )\n",
    "\n",
    "            errors = pd.concat(ErrorsDFsList)\n",
    "\n",
    "\n",
    "            #distributions = offlineMLbyPL.get_distrib_params(errors, meta=meta, metrics=metrics)\n",
    "            distribution = pd.DataFrame(np.nan, index=['thresh', 'proba', 'nvalues', 'k','loc','theta'], columns=['WeeksGroup', 'LocationId', *metrics])\n",
    "            distrib = fit_distribution_eric_local(errors, metrics)\n",
    "            for m in metrics:\n",
    "                for k in range(0,6):\n",
    "                    mLoc = distribution.columns.get_loc(m)\n",
    "                    distribution.iloc[k,mLoc] = distrib[m][k]\n",
    "\n",
    "            distribution = distribution.assign(LocationId=LocId, WeeksGroup=WGrp)\n",
    "            distribution.index.names = ['GammaParam']\n",
    "\n",
    "            #alrDF = compute_alr_eric_local(AbsErrors, GammaLawDF, metrics)\n",
    "            alrDF = compute_alr_eric_local(errors, distribution, metrics)\n",
    "\n",
    "            distribution.reset_index(inplace=True)\n",
    "            distributionsDFList.append(distribution)\n",
    "\n",
    "\n",
    "            totalALR.append(alrDF)\n",
    "\n",
    "            #break\n",
    "\n",
    "\n",
    "            pbar.update(1)\n",
    "        \n",
    "            \n",
    "distributionsDF = pd.concat(distributionsDFList)\n",
    "totalALRDF = pd.concat(totalALR)\n",
    "\n",
    "\n",
    "\n",
    "#print(distributionsDF.info())\n",
    "#print(distributionsDF.describe())\n",
    "#print(distributionsDF)\n",
    "\n",
    "\n",
    "#distributionsDF.to_parquet(path=ParquetFilesDistribsLoc, partition_cols=['WeeksGroup','LocationId'], index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#totalALRDF['time_utc'] = pd.to_datetime(totalALRDF['time_utc'])\n",
    "\n",
    "#ParquetFilesDistribsLoc = '/WORKSPACE/Pierre/tests/' + DatasetPrefix + '_' + LocationPrefix + '_distribs300LocIdsGroup012/'\n",
    "totalALRDF.to_parquet(path=ParquetFilesALRsLoc, partition_cols=['WeeksGroup', 'LocationId'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ParquetFilesALRsLoc = '/WORKSPACE/Pierre/tests/' + DatasetPrefix + '_' + LocationPrefix + '_ALR0fill300LocIdsGroup012/'\n",
    "distributionsDF.to_parquet(path=ParquetFilesDistribsLoc, partition_cols=['WeeksGroup', 'LocationId'], index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:36579)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/rapids/lib/python3.7/site-packages/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/rapids/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server (127.0.0.1:36579)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1116\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-9fc505ae294f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# compensate for the fact that we have to compute the totalALDRF thing in 3 different times to store the result...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtotalALRDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParquetFilesALRsLoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdistributionsDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParquetFilesDistribsLoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataFrameReader\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \"\"\"\n\u001b[0;32m--> 755\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrameReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spark)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1029\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \"\"\"\n\u001b[0;32m-> 1031\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    983\u001b[0m         connection = GatewayConnection(\n\u001b[1;32m    984\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[0;32m--> 985\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1125\u001b[0m                 \u001b[0;34m\"server ({0}:{1})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_authenticate_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:36579)"
     ]
    }
   ],
   "source": [
    "# compensate for the fact that we have to compute the totalALDRF thing in 3 different times to store the result...\n",
    "\n",
    "totalALRDF = spark.read.parquet(ParquetFilesALRsLoc).toPandas()\n",
    "\n",
    "distributionsDF = spark.read.parquet(ParquetFilesDistribsLoc).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6156 entries, 0 to 5\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   GammaParam   6156 non-null   object \n",
      " 1   WeeksGroup   6156 non-null   int64  \n",
      " 2   LocationId   6156 non-null   int64  \n",
      " 3   Voice        3352 non-null   float64\n",
      " 4   SMS_3G       3352 non-null   float64\n",
      " 5   PS           3352 non-null   float64\n",
      " 6   CS           3352 non-null   float64\n",
      " 7   Call         3860 non-null   float64\n",
      " 8   SMS_4G       3860 non-null   float64\n",
      " 9   Service_Req  3857 non-null   float64\n",
      " 10  HO           3860 non-null   float64\n",
      "dtypes: float64(8), int64(2), object(1)\n",
      "memory usage: 577.1+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 93078720 entries, 0 to 10079\n",
      "Data columns (total 13 columns):\n",
      " #   Column            Dtype  \n",
      "---  ------            -----  \n",
      " 0   MinuteWithinWeek  int64  \n",
      " 1   Voice             float64\n",
      " 2   PS                float64\n",
      " 3   SMS_3G            float64\n",
      " 4   CS                float64\n",
      " 5   Service_Req       float64\n",
      " 6   Call              float64\n",
      " 7   SMS_4G            float64\n",
      " 8   HO                float64\n",
      " 9   WeeksGroup        int64  \n",
      " 10  WeekOfYear        int64  \n",
      " 11  LocationId        int64  \n",
      " 12  ALR               float64\n",
      "dtypes: float64(9), int64(4)\n",
      "memory usage: 9.7 GB\n"
     ]
    }
   ],
   "source": [
    "distributionsDF.info(verbose=True)\n",
    "\n",
    "totalALRDF.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def level1byPL(alr:pd.Series) -> float:\n",
    "    \"\"\"Sets a level 1 ALR threshold (i.e. pre-alert threshold) for the input data series.\n",
    "    The threshold corresponds to the 2-sigma quantile of the ALR distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alr: pandas.core.frame.Series\n",
    "        The series of ALR values\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    thresh: float\n",
    "        The ALR threshold of the series\n",
    "    \"\"\"\n",
    "\n",
    "    #res = alr.replace([np.inf, -np.inf], np.nan).dropna(how=\"all\")\n",
    "    # 1 alert every 22 minutes was the parameter used by \n",
    "    # once per 4 hours -> 1 per 4*60 minutes\n",
    "    #return res.quantile(1/240)\n",
    "    return alr.quantile(0.00416667)\n",
    "\n",
    "def level2byPL(alr:pd.Series) -> float:\n",
    "    # 1 alert every 370 minutes\n",
    "    \"\"\"Sets a level 2 ALR threshold (i.e. alert threshold) for the input data series.\n",
    "    The threshold corresponds to the 3-sigma quantile of the ALR distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alr: pandas.core.frame.Series\n",
    "        The series of ALR values\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    thresh: float\n",
    "        The ALR threshold of the series\n",
    "    \"\"\"\n",
    "\n",
    "    #res = alr.replace([np.inf, -np.inf], np.nan).dropna(how=\"all\")\n",
    "    #return res.quantile(1-0.9973)\n",
    "    # once per day -> 1 per 24*60 minutes\n",
    "    #return res.quantile(1/1440)\n",
    "    return alr.quantile(0.0006944)\n",
    "\n",
    "def level3byPL(alr:pd.Series) -> float:\n",
    "    \"\"\"Sets a level 3 ALR threshold (i.e. maximal alert threshold) for the input data series.\n",
    "    The threshold corresponds to the 4-sigma quantile of the ALR distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alr: pandas.core.frame.Series\n",
    "        The series of ALR values\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    thresh: float\n",
    "        The ALR threshold of the series\n",
    "    \"\"\"\n",
    "    #1 alert every 15873 minute\n",
    "    #res = alr.replace([np.inf, -np.inf], np.nan).dropna(how=\"all\")\n",
    "    #return res.quantile(1-0.999937)\n",
    "    return alr.quantile(0.0000992)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#CsvFilesThresholdsLoc = 'exports/' + DatasetPrefix + '_thresholds_0fill2groups_' + LocationPrefix + '.csv'\n",
    "\n",
    "\n",
    "# apply the offlineML functions to aggregate thresholds for each antenna\n",
    "thresholds = totalALRDF.groupby(['WeeksGroup','LocationId']).ALR.agg([level1byPL, level2byPL, level3byPL])\n",
    "thresholds.reset_index(inplace=True)\n",
    "\n",
    "# don't forget to store the index or the following will be annoying\n",
    "#thresholds.reset_index(inplace=True)\n",
    "#thresholds.to_csv(CsvFilesThresholdsLoc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CsvFilesThresholdsLoc = '/WORKSPACE/Pierre/tests/' + DatasetPrefix + '_' + LocationPrefix + '_thresholds_300LocIdsGroup012.csv'\n",
    "thresholds.to_csv(CsvFilesThresholdsLoc, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39 entries, 0 to 38\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   WeeksGroup  39 non-null     int64  \n",
      " 1   LocationId  39 non-null     int64  \n",
      " 2   level1byPL  39 non-null     float64\n",
      " 3   level2byPL  39 non-null     float64\n",
      " 4   level3byPL  39 non-null     float64\n",
      "dtypes: float64(3), int64(2)\n",
      "memory usage: 1.6 KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WeeksGroup</th>\n",
       "      <th>LocationId</th>\n",
       "      <th>level1byPL</th>\n",
       "      <th>level2byPL</th>\n",
       "      <th>level3byPL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>39.00000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>11.692308</td>\n",
       "      <td>-3.677790</td>\n",
       "      <td>-6.483696</td>\n",
       "      <td>-10.334911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.82717</td>\n",
       "      <td>4.840367</td>\n",
       "      <td>3.370149</td>\n",
       "      <td>5.777767</td>\n",
       "      <td>9.220322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-14.405279</td>\n",
       "      <td>-24.114274</td>\n",
       "      <td>-34.136648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>-5.526967</td>\n",
       "      <td>-9.248671</td>\n",
       "      <td>-14.862737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>-4.144289</td>\n",
       "      <td>-7.138496</td>\n",
       "      <td>-11.536166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.00000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.00000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       WeeksGroup  LocationId  level1byPL  level2byPL  level3byPL\n",
       "count    39.00000   39.000000   39.000000   39.000000   39.000000\n",
       "mean      1.00000   11.692308   -3.677790   -6.483696  -10.334911\n",
       "std       0.82717    4.840367    3.370149    5.777767    9.220322\n",
       "min       0.00000    4.000000  -14.405279  -24.114274  -34.136648\n",
       "25%       0.00000    8.000000   -5.526967   -9.248671  -14.862737\n",
       "50%       1.00000   12.000000   -4.144289   -7.138496  -11.536166\n",
       "75%       2.00000   16.000000    0.000000    0.000000    0.000000\n",
       "max       2.00000   19.000000    0.000000    0.000000    0.000000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(thresholds.info(verbose=True))\n",
    "thresholds.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3078 entries, 5 to 5\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   GammaParam   3078 non-null   object \n",
      " 1   WeeksGroup   3078 non-null   int64  \n",
      " 2   LocationId   3078 non-null   int64  \n",
      " 3   Voice        976 non-null    float64\n",
      " 4   SMS_3G       976 non-null    float64\n",
      " 5   PS           977 non-null    float64\n",
      " 6   CS           976 non-null    float64\n",
      " 7   Call         1342 non-null   float64\n",
      " 8   SMS_4G       1343 non-null   float64\n",
      " 9   Service_Req  1310 non-null   float64\n",
      " 10  HO           1343 non-null   float64\n",
      "dtypes: float64(8), int64(2), object(1)\n",
      "memory usage: 288.6+ KB\n",
      "\n",
      "\n",
      "proba statistics\n",
      "             Voice       SMS_3G           PS           CS         Call       SMS_4G  Service_Req           HO\n",
      "count  3078.000000  3078.000000  3078.000000  3078.000000  3078.000000  3078.000000  3078.000000  3078.000000\n",
      "mean      0.013033     0.009380     0.008660     0.008788     0.014187     0.011557     0.007283     0.009693\n",
      "std       0.020373     0.014797     0.013689     0.014179     0.017208     0.014215     0.013529     0.012834\n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000     0.000000     0.000000     0.000000\n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000     0.000000     0.000000     0.000000\n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000     0.000000     0.000000     0.000000\n",
      "75%       0.031457     0.021558     0.020102     0.019568     0.031235     0.025169     0.008083     0.019583\n",
      "max       0.084841     0.049544     0.060060     0.074603     0.078373     0.053571     0.079216     0.052302\n",
      "\n",
      "\n",
      "possible non-zero values depending on the technology\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SelectedLocIdsInfos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-1e90f52fa2eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\npossible non-zero values depending on the technology\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mCountTechno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelectedLocIdsInfos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LocationId'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TECHNO'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TECHNO'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCountTechno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SelectedLocIdsInfos' is not defined"
     ]
    }
   ],
   "source": [
    "#print(LocIdsList)\n",
    "# some verifications about what we have in the \n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "distributionsDF[distributionsDF['GammaParam']=='theta'].info()\n",
    "\n",
    "#print(\"threshold statistics\")\n",
    "#distributionsDF[distributionsDF['GammaParam']=='thresh'].drop(columns=['LocationId', 'WeeksGroup']).describe()\n",
    "\n",
    "print(\"\\n\\nproba statistics\")\n",
    "print(distributionsDF[distributionsDF['GammaParam']=='proba'].drop(columns=['LocationId', 'WeeksGroup']).describe())\n",
    "\n",
    "LocIdsInfosDF = pd.read_csv('exports/Cancan_Paris_LocInfos_AllData.csv')\n",
    "#print(LocIdsInfosDF.info(verbose=True))\n",
    "\n",
    "#SelectedLocIdsInfos = LocIdsInfosDF[LocIdsInfosDF['LocationId'].isin(SmallerLocIdsList)]\n",
    "#print(SelectedLocIdsInfos.info(verbose=True))\n",
    "\n",
    "print(\"\\n\\npossible non-zero values depending on the technology\")\n",
    "CountTechno = SelectedLocIdsInfos.groupby(['LocationId', 'TECHNO']).size().groupby('TECHNO').count()\n",
    "print(CountTechno)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationId</th>\n",
       "      <th>WeeksGroup</th>\n",
       "      <th>level1byPL</th>\n",
       "      <th>level2byPL</th>\n",
       "      <th>level3byPL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.0</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1061.803333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.195179</td>\n",
       "      <td>-11.174876</td>\n",
       "      <td>-18.086576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>110.215135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.778842</td>\n",
       "      <td>6.997304</td>\n",
       "      <td>9.384837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>875.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-26.323358</td>\n",
       "      <td>-56.610089</td>\n",
       "      <td>-65.210885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>961.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.574128</td>\n",
       "      <td>-14.145312</td>\n",
       "      <td>-22.871932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1064.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.398969</td>\n",
       "      <td>-9.676692</td>\n",
       "      <td>-16.536602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1156.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.668047</td>\n",
       "      <td>-6.774801</td>\n",
       "      <td>-11.970358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1251.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000121</td>\n",
       "      <td>-0.000121</td>\n",
       "      <td>-0.000123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        LocationId  WeeksGroup  level1byPL  level2byPL  level3byPL\n",
       "count   300.000000       300.0  300.000000  300.000000  300.000000\n",
       "mean   1061.803333         0.0   -5.195179  -11.174876  -18.086576\n",
       "std     110.215135         0.0    3.778842    6.997304    9.384837\n",
       "min     875.000000         0.0  -26.323358  -56.610089  -65.210885\n",
       "25%     961.750000         0.0   -6.574128  -14.145312  -22.871932\n",
       "50%    1064.000000         0.0   -4.398969   -9.676692  -16.536602\n",
       "75%    1156.500000         0.0   -2.668047   -6.774801  -11.970358\n",
       "max    1251.000000         0.0   -0.000121   -0.000121   -0.000123"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
